{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../datasets/cifar-10-batches-py/\n",
      "chargement terminé\n",
      "donnée d'entrainement : (5000, 3, 32, 32)\n",
      "donnée de validation : (1000, 3, 32, 32)\n",
      "donnée de test : (500, 32, 32, 3)\n",
      "Temps : 0.03982877731323242\n",
      "echantillonage pour kmeans 0 / 400000\n",
      "echantillonage pour kmeans 10000 / 400000\n",
      "echantillonage pour kmeans 20000 / 400000\n",
      "echantillonage pour kmeans 30000 / 400000\n",
      "echantillonage pour kmeans 40000 / 400000\n",
      "echantillonage pour kmeans 50000 / 400000\n",
      "echantillonage pour kmeans 60000 / 400000\n",
      "echantillonage pour kmeans 70000 / 400000\n",
      "echantillonage pour kmeans 80000 / 400000\n",
      "echantillonage pour kmeans 90000 / 400000\n",
      "echantillonage pour kmeans 100000 / 400000\n",
      "echantillonage pour kmeans 110000 / 400000\n",
      "echantillonage pour kmeans 120000 / 400000\n",
      "echantillonage pour kmeans 130000 / 400000\n",
      "echantillonage pour kmeans 140000 / 400000\n",
      "echantillonage pour kmeans 150000 / 400000\n",
      "echantillonage pour kmeans 160000 / 400000\n",
      "echantillonage pour kmeans 170000 / 400000\n",
      "echantillonage pour kmeans 180000 / 400000\n",
      "echantillonage pour kmeans 190000 / 400000\n",
      "echantillonage pour kmeans 200000 / 400000\n",
      "echantillonage pour kmeans 210000 / 400000\n",
      "echantillonage pour kmeans 220000 / 400000\n",
      "echantillonage pour kmeans 230000 / 400000\n",
      "echantillonage pour kmeans 240000 / 400000\n",
      "echantillonage pour kmeans 250000 / 400000\n",
      "echantillonage pour kmeans 260000 / 400000\n",
      "echantillonage pour kmeans 270000 / 400000\n",
      "echantillonage pour kmeans 280000 / 400000\n",
      "echantillonage pour kmeans 290000 / 400000\n",
      "echantillonage pour kmeans 300000 / 400000\n",
      "echantillonage pour kmeans 310000 / 400000\n",
      "echantillonage pour kmeans 320000 / 400000\n",
      "echantillonage pour kmeans 330000 / 400000\n",
      "echantillonage pour kmeans 340000 / 400000\n",
      "echantillonage pour kmeans 350000 / 400000\n",
      "echantillonage pour kmeans 360000 / 400000\n",
      "echantillonage pour kmeans 370000 / 400000\n",
      "echantillonage pour kmeans 380000 / 400000\n",
      "echantillonage pour kmeans 390000 / 400000\n",
      "time 0.2563927412033081\n",
      "Blanchiment\n",
      "time 0.2693758845329285\n",
      "kmeans iters 1 / 50\n",
      "kmeans iters 2 / 50\n",
      "kmeans iters 3 / 50\n",
      "kmeans iters 4 / 50\n",
      "kmeans iters 5 / 50\n",
      "kmeans iters 6 / 50\n",
      "kmeans iters 7 / 50\n",
      "kmeans iters 8 / 50\n",
      "kmeans iters 9 / 50\n",
      "kmeans iters 10 / 50\n",
      "kmeans iters 11 / 50\n",
      "kmeans iters 12 / 50\n",
      "kmeans iters 13 / 50\n",
      "kmeans iters 14 / 50\n",
      "kmeans iters 15 / 50\n",
      "kmeans iters 16 / 50\n",
      "kmeans iters 17 / 50\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from data_utils import load_CIFAR10\n",
    "from reseau import *\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os.path\n",
    "warnings.filterwarnings(\"ignore\", \"Mean of empty slice\")\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def get_CIFAR10_data(num_training = 5000, num_validation = 1000, num_test = 500):\n",
    "    \"\"\" On télécharge ici à partir du dossier et on prepare les données à être recu par le reseau de neuronne\n",
    "  \n",
    "    \"\"\"\n",
    "\n",
    "    # Chargerment des données brutes.\n",
    "    cifar10_dir = '../../datasets/cifar-10-batches-py/'\n",
    "    print(cifar10_dir)\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "    # Sous ensemble des données\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalisation des données, on soustrait la moyenne.\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val = X_val - mean_image\n",
    "    X_test = X_test - mean_image\n",
    "    X_train = X_train.swapaxes(1,3)\n",
    "    X_val = X_val.swapaxes(1,3)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print(\"chargement terminé\")\n",
    "print(\"donnée d'entrainement :\" , X_train.shape)\n",
    "print(\"donnée de validation :\" , X_val.shape)\n",
    "print(\"donnée de test :\" , X_test.shape)\n",
    "print(\"Temps :\" , (time.time()-start_time)/60)\n",
    "\n",
    "\n",
    "\n",
    "rec_field_size = 6\n",
    "nombre_centroide = 1600\n",
    "Blanchement = True\n",
    "nombre_patches = 400000\n",
    "DIM_IMG = [32,32,3]\n",
    "\n",
    "\n",
    "\n",
    "#création des patches\n",
    "patches = []\n",
    "for i in range(nombre_patches):\n",
    "    if(np.mod(i, 10000)== 0):\n",
    "        print(\"echantillonage pour kmeans\",i,\"/\", nombre_patches)\n",
    "    start_r = np.random.randint(DIM_IMG[0] - rec_field_size)\n",
    "    start_c = np.random.randint(DIM_IMG[1] - rec_field_size)\n",
    "    patch = np.array([])\n",
    "    img = X_train[np.mod(i, X_train.shape[0])]\n",
    "    for layer in img:\n",
    "        patch = np.append(patch, layer[start_r:start_r + rec_field_size].T[start_c:start_c + rec_field_size].T.ravel())\n",
    "    patches.append(patch)\n",
    "patches = np.array(patches)\n",
    "\n",
    "#on normalise les patches\n",
    "patches = (patches-patches.mean(1)[:,None])/np.sqrt(patches.var(1)+ 10)[:, None]\n",
    "print(\"time\", (time.time()-start_time)/60)\n",
    "\n",
    "del X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "#blanchiment\n",
    "print(\"Blanchiment\")\n",
    "[D,V]= np.linalg.eig(np.cov(patches, rowvar = 0))\n",
    "P = V.dot(np.diag(np.sqrt(1/(D + 0.1)))).dot(V.T)\n",
    "patches = patches.dot(P)\n",
    "\n",
    "print(\"time\", (time.time() - start_time)/60.0)\n",
    "del D,V\n",
    "#Application de K-means sur les patches\n",
    "centroids = np.random.randn(nombre_centroide, patches.shape[1])*.1\n",
    "num_iters = 50\n",
    "batch_size = 128\n",
    "for ite in range(num_iters):\n",
    "    print(\"kmeans iters\", ite+1,\"/\", num_iters )\n",
    "    hf_c2_sum = .5*np.power(centroids, 2).sum(1)\n",
    "    counts = np.zeros(nombre_centroide)\n",
    "    summation = np.zeros_like(centroids)\n",
    "    for i in range(0, len(patches), batch_size):\n",
    "        last_i = min(i+batch_size, len(patches))\n",
    "        idx = np.argmax(patches[i:last_i].dot(centroids.T) -hf_c2_sum.T, axis = 1)\n",
    "        S = np.zeros([last_i - i, nombre_centroide])\n",
    "        S[range(last_i-i), np.argmax(patches[i:last_i].dot(centroids.T)-hf_c2_sum.T, axis=1)]=1\n",
    "        summation+=S.T.dot(patches[i:last_i])\n",
    "        counts+= S.sum(0)\n",
    "    centroids = summation/counts[:,None]\n",
    "    centroids[counts==0]=0\n",
    "\n",
    "print(\"time\", (time.time()-start_time)/60.0)\n",
    "\n",
    "\n",
    "\n",
    "def sliding(img, window=[6,6]):\n",
    "    \"\"\" fonction qui permettrait le decoupage en patch des images.  \"\"\"\n",
    "    out = np.array([])\n",
    "    for i in range(3):\n",
    "        s = img.shape\n",
    "        row = s[1]\n",
    "        col = s[2]\n",
    "        col_extent = col - window[1]+ 1\n",
    "        row_extent = row - window[0]+ 1\n",
    "        start_idx = np.arange(window[0])[:,None]*col + np.arange(window[1])\n",
    "        offset_idx = np.arange(row_extent)[:,None]*col + np.arange(col_extent)\n",
    "        if len(out)==0:\n",
    "            out = np.take(img[i],start_idx.ravel()[:,None] + offset_idx.ravel())\n",
    "        else:\n",
    "            out=np.append(out,np.take(img[i], start_idx.ravel()[:,None] + offset_idx.ravel()),axis=0)\n",
    "    return out\n",
    "\n",
    "\n",
    "#extraction des features.\n",
    "def extract_features(X_train):\n",
    "    trainXC = []\n",
    "    idx = 0\n",
    "    for img in X_train:\n",
    "        idx += 1\n",
    "        if not np.mod(idx,1000):\n",
    "            print('extract feature', idx, \"/\", len(X_train))\n",
    "            print(\"time\", (time.time()-start_time)/60)\n",
    "        patches = sliding(img,[rec_field_size, rec_field_size]).T\n",
    "        #on normalise\n",
    "        patches = (patches-patches.mean(1)[:,None])/(np.sqrt(patches.var(1)+0.1)[:,None])\n",
    "        patches = patches.dot(P)\n",
    "\n",
    "        x2 = np.power(patches,2).sum(1)\n",
    "        c2 = np.power(centroids,2).sum(1)\n",
    "        xc = patches.dot(centroids.T)\n",
    "\n",
    "        dist = np.sqrt(-2*xc+x2[:,None] + c2)\n",
    "        u = dist.mean(1)\n",
    "        #f_k(x) = max{0, mu(z) - z_k}\n",
    "        patches = np.maximum(-dist+u[:, None],0)\n",
    "        rs = DIM_IMG[0]-rec_field_size+1\n",
    "        cs = DIM_IMG[1]-rec_field_size+1\n",
    "        patches = np.reshape(patches, [rs, cs, -1])\n",
    "        q = []\n",
    "        q.append(patches[0:int(rs/2), 0:int(cs/2)].sum(0).sum(0))\n",
    "        q.append(patches[0:int(rs/2), int(cs/2):int(cs-1)].sum(0).sum(0))\n",
    "        q.append(patches[int(rs/2):int(rs-1),0:int(cs/2)].sum(0).sum(0))\n",
    "        q.append(patches[int(rs/2):int(rs-1),int(cs/2):int(cs-1)].sum(0).sum(0))\n",
    "        q = np.array(q).ravel()\n",
    "        trainXC.append(q)\n",
    "\n",
    "    trainXC = np.array(trainXC)\n",
    "    \n",
    "    trainXC=(trainXC-trainXC.mean(1)[:,None])/(np.sqrt(trainXC.var(1)+10)[:,None])\n",
    "    return trainXC\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "trainXC = extract_features(X_train)\n",
    "\n",
    "print(\"time\", (time.time()-start_time)/60.0)\n",
    "valXC = extract_features(X_val)\n",
    "testXC = extract_features(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_size = trainXC.shape[1]\n",
    "hidden_size = 300\n",
    "num_classes = 10\n",
    "\n",
    "net = TwoLayerNet(input_size, hidden_size, num_classes,1e-4)\n",
    "stats = net.train(trainXC, y_train, valXC, y_val,num_iters=10000, batch_size=128,learning_rate=5e-4, learning_rate_decay=0.99,reg=0, verbose=True,update=\"momentum\",arg=0.95,dropout=0.5)\n",
    "\n",
    "\n",
    "val_acc = (net.predict(trainXC) == y_train).mean()\n",
    "print ('Précision sur les donnéés dentrainement: ', val_acc)\n",
    "val_acc = (net.predict(valXC) == y_val).mean()\n",
    "print ('Précision réel: ', val_acc)\n",
    "\n",
    "print (\"time\",(time.time()-start_time)/60.0)\n",
    "\n",
    "\n",
    "##Plot the loss function and train / validation accuracies\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "##plt.savefig(\"dropout loss_history.eps\")\n",
    "\n",
    "plt.plot(stats['train_acc_history'], label='train')\n",
    "plt.plot(stats['val_acc_history'], label='val')\n",
    "plt.title('Classification accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()\n",
    "plt.ylabel('Clasification accuracy')\n",
    "plt.savefig('dropout accuracy.eps')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python37564bit58e26b6ca97d4fcfae909effff83e9fb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
